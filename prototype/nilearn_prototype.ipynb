{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b62a670",
   "metadata": {},
   "source": [
    "# An example workflow for generating hypotheses with ABCD data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388451e-8b5f-48fc-8c17-08eadc8cf694",
   "metadata": {},
   "source": [
    "## Installing dependent Python packages\n",
    "One way to do this is with a virtual python environment installed so that it is accessible to Jupyter lab.  This needs to be set up only once.\n",
    "```bash\n",
    "python -m venv ~/abcd311\n",
    "source ~/abcd311/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name abcd311\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Then, once Jupyter is open with this lab notebook, \"Change Kernel...\" to be \"abcd311\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be356f-c09a-433c-ae01-df4bc0a41e7f",
   "metadata": {},
   "source": [
    "## Import dependent Pyton packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a64fa-70ca-4bdf-9950-e45489c543ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Python packages\n",
    "from typing import Any, Union\n",
    "import csv\n",
    "import dipy.io.image\n",
    "import functools\n",
    "import math\n",
    "import nibabel.nifti1\n",
    "import nilearn.masking\n",
    "import nilearn.mass_univariate\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b780e2-3086-427f-a769-19deff481f64",
   "metadata": {},
   "source": [
    "## Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f03bf-1874-4b60-a1ad-b9e52c612cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global parameters to match your environment.  Ultimately these will be member variables of a class\n",
    "gor_image_directory: str = \"/data2/ABCD/gor-images\"\n",
    "white_matter_mask_file: str = os.path.join(gor_image_directory, \"gortemplate0.nii.gz\")\n",
    "coregistered_images_directory: str = os.path.join(gor_image_directory, \"coregistered-images\")\n",
    "tabular_data_directory: str = \"/data2/ABCD/abcd-5.0-tabular-data-extracted\"\n",
    "core_directory: str = os.path.join(tabular_data_directory, \"core\")\n",
    "\n",
    "# Useful class static const member\n",
    "# Images are distinguished from each other by their subjects and timing.  (Also distinguished as \"md\" vs. \"fa\" type, though not relevant here.)\n",
    "join_keys: list[str] = [\"src_subject_id\", \"eventname\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50269fdf-0b81-4152-b8f8-71d5ea83ccde",
   "metadata": {},
   "source": [
    "## Define functions for various steps of the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9974ac0-0c87-463b-950c-e751cc4b2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for handling image voxel data\n",
    "\n",
    "\n",
    "def get_list_of_image_files(directory: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of full path names of image files.  Input is the directory in which to look for these files.\n",
    "    \"\"\"\n",
    "    # Choose the pattern to get .nii.gz files but avoid the template files such as gortemplate0.nii.gz\n",
    "    pattern: str = r\"^gorinput[0-9]{4}-.*\\.nii\\.gz$\"\n",
    "    response: list[str] = [\n",
    "        os.path.join(directory, file)\n",
    "        for file in os.listdir(directory)\n",
    "        if bool(re.match(pattern, file))\n",
    "    ]\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_image_filenames(list_of_image_files: list[str]) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a list of image file names, returns a pandas DataFrame.\n",
    "    The first column in the output is the filename.\n",
    "    Additional columns indicate how the filename was parsed.\n",
    "    For example, run as:\n",
    "        df = parse_image_filenames(get_list_of_image_files(coregistered_images_directory))\n",
    "    \"\"\"\n",
    "    filename_pattern: str = (\n",
    "        r\"gorinput([0-9]+)-modality([0-9]+)-sub-([A-Za-z0-9]+)_ses-([A-Za-z0-9]+)_run-([A-Za-z0-9]+)_([A-Za-z0-9]+)_([A-Za-z0-9]+)-([A-Za-z0-9]+).nii.gz\"\n",
    "    )\n",
    "    filename_keys: list[str] = [\n",
    "        \"filename\",\n",
    "        \"gorinput\",\n",
    "        \"modality\",\n",
    "        \"src_subject_id\",\n",
    "        \"eventname\",\n",
    "        \"run\",\n",
    "        \"image_type\",\n",
    "        \"image_subtype\",\n",
    "        \"processing\",\n",
    "    ]\n",
    "\n",
    "    # Parse the basename of each filename and use it to construct a row of the `response` dataframe\n",
    "    response: pd.core.frame.DataFrame = pd.DataFrame(\n",
    "        [\n",
    "            [filename, *list(re.match(filename_pattern, os.path.basename(filename)).groups())]\n",
    "            for filename in list_of_image_files\n",
    "        ],\n",
    "        columns=filename_keys,\n",
    "    )\n",
    "\n",
    "    # Fix parsing of src_subject_id.  The filenames do not have an underscore after \"NDAR\",\n",
    "    # but src_subject_id values in the data tables do.\n",
    "    response[\"src_subject_id\"] = [\n",
    "        re.sub(r\"^NDAR\", \"NDAR_\", subject) for subject in response[\"src_subject_id\"]\n",
    "    ]\n",
    "    # Fix parsing of eventname.  The filenames use CamelCase but the datatables use snake_case.\n",
    "    eventname_conversion: dict[str, str] = {\n",
    "        \"baselineYear1Arm1\": \"baseline_year_1_arm_1\",\n",
    "        \"1YearFollowUpYArm1\": \"1_year_follow_up_y_arm_1\",\n",
    "        \"2YearFollowUpYArm1\": \"2_year_follow_up_y_arm_1\",\n",
    "        \"3YearFollowUpYArm1\": \"3_year_follow_up_y_arm_1\",\n",
    "        \"4YearFollowUpYArm1\": \"4_year_follow_up_y_arm_1\",\n",
    "    }\n",
    "    response[\"eventname\"] = [eventname_conversion[event] for event in response[\"eventname\"]]\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_data_from_image_files(list_of_files: list[str]) -> list[Any]:\n",
    "    \"\"\"\n",
    "    get_data_from_image_files returns a list of tuples of 4 values.\n",
    "    The latter 3 values in the tuple are the return from load_nifti.\n",
    "        <class 'str'>: full file name\n",
    "        <class 'numpy.ndarray'>: image data as a numpy array\n",
    "        <class 'numpy.ndarray'>: some other numpy array (transform?)\n",
    "        <class 'nibabel.nifti1.Nifti1Image'>: an object that can be modified and can be written to file as a .nii.gz file\n",
    "    \"\"\"\n",
    "    response = [(file,) + dipy.io.image.load_nifti(file, return_img=True) for file in list_of_files]\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_white_matter_mask_as_numpy(\n",
    "    white_matter_mask_file: str, mask_threshold: float\n",
    ") -> np.ndarray:\n",
    "    # The return value is a np.ndarray of bool, a voxel mask that indicates which voxels are to be kept for subsequent analyses.\n",
    "    # The mask is flattened to a single dimension because our analysis software indexes voxels this way\n",
    "    # We determine white matter by looking at the white_matter_mask_file for voxels that have an intensity above a threshold\n",
    "    white_matter_mask_input: np.ndarray = get_data_from_image_files([white_matter_mask_file])[0][1]\n",
    "    white_matter_mask: np.ndarray = (white_matter_mask_input >= mask_threshold).reshape(-1)\n",
    "    print(f\"Number of white matter voxels = {np.sum(white_matter_mask)}\")\n",
    "    return white_matter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4ff2a-16c3-4183-85f0-35f0d3647c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reading and selecting data from csv files\n",
    "\n",
    "\n",
    "def csv_file_to_dataframe(filename: str) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    A stupid function that reminds us how to read a csv file using pandas\n",
    "    \"\"\"\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "def select_rows_of_dataframe(df: pd.core.frame.DataFrame, query_dict: dict[str, str]):\n",
    "    \"\"\"\n",
    "    This function is deprecated in favor of using pandas `isin` functionality.\n",
    "    \"\"\"\n",
    "    # Each key of query_dict is a column header of the df dataframe.\n",
    "    # Each value of query_dict is a list of allowed values.\n",
    "    # A row will be selected only if each of these columns has one of the allowed keys\n",
    "    assert all(key in df.columns for key in query_dict.keys())\n",
    "    # Old code: each of query_dict.values() is just a single value, not a list of values:\n",
    "    # rows = df[\n",
    "    #     functools.reduce(lambda x, y: x & y, [df[key] = value for key, value in query_dict.items()])\n",
    "    # ]\n",
    "    rows: pd.core.frame.DataFrame = df[\n",
    "        functools.reduce(\n",
    "            lambda x, y: x & y,\n",
    "            [\n",
    "                functools.reduce(lambda x, y: x | y, [df[key] == value for value in values])\n",
    "                for key, values in query_dict.items()\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714624c-a1c2-4b80-875b-fc156ff55b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read and cache KSADS tabular information\n",
    "\n",
    "\n",
    "def clean_ksads_data_frame(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    KSADS data uses:\n",
    "        1 = present\n",
    "        0 = absent\n",
    "        888 = Question not asked due to primary question response (branching logic)\n",
    "        555 = Not administered in the assessment\n",
    "    At least for now, we will assume:\n",
    "        that 888 means that the question was not asked because the answer was already known to be \"absent\".\n",
    "        that 555 means we don't know the answer.\n",
    "    See https://wiki.abcdstudy.org/release-notes/non-imaging/mental-health.html\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if bool(re.match(\"ksads_\\d\", column)):\n",
    "            df[column] = df[column].astype(float)\n",
    "            df.loc[df[column] == 555, column] = np.nan\n",
    "            df.loc[df[column] == 888, column] = 0\n",
    "    return df\n",
    "\n",
    "\n",
    "def ksads_filename_to_dataframe(\n",
    "    file_mh_y_ksads_ss: str, use_cache: bool = True\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in the KSADS data, or grab it from cache if the user requests it and we have it.\n",
    "    \"\"\"\n",
    "    rebuild_cache: bool = not use_cache\n",
    "    try:\n",
    "        # Check whether we've cached this computation\n",
    "        ksads_filename_to_dataframe.df_mh_y_ksads_ss\n",
    "    except AttributeError:\n",
    "        # We haven't cached it, so we'll have to read the data even if the user would have permitted us to use the cache\n",
    "        rebuild_cache = True\n",
    "    if rebuild_cache:\n",
    "        print(\"Begin reading KSADS data file\")\n",
    "        start: float = time.time()\n",
    "        # Reading from disk takes 10-30 seconds; it is a big file\n",
    "        response: pd.core.frame.DataFrame = csv_file_to_dataframe(file_mh_y_ksads_ss)\n",
    "        # Deal with 555 and 888 values in the data table\n",
    "        response = clean_ksads_data_frame(response)\n",
    "        # Place the computed table in the cache\n",
    "        ksads_filename_to_dataframe.df_mh_y_ksads_ss = response\n",
    "        print(f\"Read KSADS data file in {time.time()-start}s\")\n",
    "    # Return what is now in the cache\n",
    "    return ksads_filename_to_dataframe.df_mh_y_ksads_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbe410-cca3-4ce5-801b-1ab20ac5353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for computing summary statistics for KSADS csv data\n",
    "\n",
    "\n",
    "def data_frame_value_counts(df: pd.core.frame.DataFrame) -> dict[str, dict[str, np.int64]]:\n",
    "    \"\"\"\n",
    "    Whether an KSADS column of data is interesting depends upon, in part, how much it varies across images.\n",
    "    Here we performa census that counts how many times each value occurs in a column.\n",
    "    \"\"\"\n",
    "    # Returns a dict:\n",
    "    #     Each key is a column name\n",
    "    #     Each value is a dict:\n",
    "    #         Each key of this is a value that occurs in the column.\n",
    "    #         The corresponding value is the number of occurrences.\n",
    "    response: dict[str, dict[str, np.int64]] = {\n",
    "        column: dict(df[column].value_counts(dropna=False).astype(int)) for column in df.columns\n",
    "    }\n",
    "    return response\n",
    "\n",
    "\n",
    "def entropy_of_column_counts(column_counts) -> float:\n",
    "    \"\"\"\n",
    "    Whether an KSADS column of data is interesting depends upon, in part, how much it varies across images.\n",
    "    Here we compute the entropy (information) of one column given its census data from data_frame_value_counts().\n",
    "    \"\"\"\n",
    "    assert all(value >= 0 for value in column_counts.values())\n",
    "    total_count = sum(column_counts.values())\n",
    "    entropy: float = sum(\n",
    "        [\n",
    "            count / total_count * math.log2(total_count / count)\n",
    "            for count in column_counts.values()\n",
    "            if count > 0\n",
    "        ]\n",
    "    )\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def ksads_keys_only(all_columns: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    This finds those keys that are KSADS variables rather than being index keys (subject, event), etc.\n",
    "    \"\"\"\n",
    "    return {key: value for key, value in all_columns.items() if bool(re.match(\"ksads_\\d\", key))}\n",
    "\n",
    "\n",
    "def entropy_of_all_columns(all_columns) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute entropy (information) of every column by calling subroutine for each column\n",
    "    \"\"\"\n",
    "    return {key: entropy_of_column_counts(value) for key, value in all_columns.items()}\n",
    "\n",
    "\n",
    "def find_interesting_entropies(file_mh_y_ksads_ss: str):\n",
    "    \"\"\"\n",
    "    Compute the entropy (information) of each KSADS variable and return them sorted from most entropy to least\n",
    "    \"\"\"\n",
    "    # Find some KSADS data columns with high entropy.\n",
    "    df_mh_y_ksads_ss: pd.core.frame.DataFrame = ksads_filename_to_dataframe(file_mh_y_ksads_ss)\n",
    "    counts_for_each_column_mh_y_ksads_ss: dict[str, Any] = ksads_keys_only(\n",
    "        data_frame_value_counts(df_mh_y_ksads_ss)\n",
    "    )\n",
    "    print(\"Column counting done\")\n",
    "\n",
    "    entropies: dict[str, float] = entropy_of_all_columns(counts_for_each_column_mh_y_ksads_ss)\n",
    "    sorted_entropies: dict[str, Any] = dict(\n",
    "        sorted(entropies.items(), key=lambda item: item[1], reverse=True)\n",
    "    )\n",
    "    sorted_entropies = {\n",
    "        key: (value, counts_for_each_column_mh_y_ksads_ss[key])\n",
    "        for key, value in sorted_entropies.items()\n",
    "        # if bool(re.match(\"ksads_\\d\", key))\n",
    "    }\n",
    "    print(\"Entropy calculation done\")\n",
    "    return sorted_entropies\n",
    "\n",
    "\n",
    "def find_interesting_ksads() -> tuple[str, list[str]]:\n",
    "    \"\"\"\n",
    "    The first returned value is the filename for the KSADS data table\n",
    "    The second returned value is computed via:\n",
    "        reading in the KSADS data,\n",
    "        computing the entropy of each column,\n",
    "        sorting entropies in decreasing order\n",
    "        choosing just the top few\n",
    "    or:\n",
    "        just use values we've computed using this process in the past\n",
    "    \"\"\"\n",
    "    file_mh_y_ksads_ss: str = \"mental-health/mh_y_ksads_ss.csv\"\n",
    "    if True:\n",
    "        full_path = os.path.join(core_directory, file_mh_y_ksads_ss)\n",
    "        sorted_entropies = find_interesting_entropies(full_path)\n",
    "        number_wanted = 4  # TODO: Is 20 a good number?\n",
    "        interesting_ksads = list(sorted_entropies.keys())[:number_wanted]\n",
    "    else:\n",
    "        # With \"555\" and \"888\" both going to \"\", the interesting_ksads computation gives:\n",
    "        interesting_ksads = [\n",
    "            \"ksads_22_142_t\",  # Symptom - Insomnia, Past\n",
    "            \"ksads_22_970_t\",  # Diagnosis - SLEEP PROBLEMS, Past\n",
    "            \"ksads_2_11_t\",  # Symptom - Explosive Irritability, Past\n",
    "            \"ksads_1_2_t\",  # Symptom - Depressed Mood, Past\n",
    "            \"ksads_2_13_t\",  # Symptom - Decreased Need for Sleep, Past\n",
    "            \"ksads_1_6_t\",  # Symptom - Anhedonia, Past\n",
    "            \"ksads_22_141_t\",  # Symptom - Insomnia, Present\n",
    "            \"ksads_22_969_t\",  # Diagnosis - SLEEP PROBLEMS, Present\n",
    "            \"ksads_2_8_t\",  # Symptom - Elevated Mood, Past\n",
    "            \"ksads_10_46_t\",  # Symptom - Excessive worries more days than not Past\n",
    "            \"ksads_1_4_t\",  # Symptom - Irritability, Past\n",
    "            \"ksads_2_10_t\",  # Symptom - ExplosiveIrritability, PresentNext\n",
    "            \"ksads_8_31_t\",  # Symptom - Fear of Social Situations, Past\n",
    "            \"ksads_23_146_t\",  # Symptom - Wishes/Better off dead, Past\n",
    "            \"ksads_23_957_t\",  # Diagnosis - SuicidalideationPassivePast\n",
    "            \"ksads_1_5_t\",  # Symptom - Anhedonia, Present\n",
    "            \"ksads_2_839_t\",  # Diagnosis - Unspecified Bipolar and Related Disorder, PAST (F31.9)\n",
    "            \"ksads_2_833_t\",  # Diagnosis - Bipolar I Disorder, most recent past episode manic (F31.1x)\n",
    "            \"ksads_1_3_t\",  # Symptom - Irritability, Present\n",
    "            \"ksads_1_842_t\",  # Diagnosis - Major Depressive Disorder, Past (F32.9)\n",
    "        ]\n",
    "        # With \"555\" going to \"\" and \"888\" going to \"0\", the interesting_ksads computation gives:\n",
    "        interesting_ksads = [\n",
    "            \"ksads_1_187_t\",  # Symptom - No two month symptom-free interval, Present\n",
    "            \"ksads_1_188_t\",  # Symptom - No two month symptom-free interval, Past\n",
    "            \"ksads_22_142_t\",  # Symptom - Insomnia, Past\n",
    "            \"ksads_22_970_t\",  # Diagnosis - SLEEP PROBLEMS, Past\n",
    "            \"ksads_2_11_t\",  # Symptom - Explosive Irritability, Past\n",
    "            \"ksads_2_222_t\",  # Symptom - Lasting at least 4 days, Past\n",
    "            \"ksads_1_184_t\",  # Symptom - Impairment in functioning due to depression, Past\n",
    "            \"ksads_1_2_t\",  # Symptom - Depressed Mood, Past\n",
    "            \"ksads_2_13_t\",  # Symptom - Decreased Need for Sleep, Past\n",
    "            \"ksads_1_6_t\",  # Symptom - Anhedonia, Past\n",
    "            \"ksads_1_160_t\",  # Symptom - Fatigue, Past\n",
    "            \"ksads_1_162_t\",  # Symptom - Concentration Disturbance, Past\n",
    "            \"ksads_2_220_t\",  # Symptom - Lasting at least one week, Past\n",
    "            \"ksads_1_156_t\",  # Symptom - Insomnia when depressed, Past\n",
    "            \"ksads_1_174_t\",  # Symptom - Psychomotor Agitation in Depressive Disorder, Past\n",
    "            \"ksads_2_216_t\",  # Symptom - Impairment in functioning due to bipolar, Past\n",
    "            \"ksads_2_208_t\",  # Symptom - Psychomotor Agitation in Bipolar Disorder, Past\n",
    "            \"ksads_2_206_t\",  # Symptom - Increased Energy, Past\n",
    "            \"ksads_22_141_t\",  # Symptom - Insomnia, Present\n",
    "            \"ksads_22_969_t\",  # Diagnosis - SLEEP PROBLEMS, Present\n",
    "        ]\n",
    "    ksads_vars: tuple[str, list[str]] = (file_mh_y_ksads_ss, interesting_ksads)\n",
    "    return ksads_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ab48b-a004-4f7f-aa4f-a17d953b5e62",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Find all images for which we have tabular data\n",
    "\n",
    "\n",
    "def get_table_drop_nulls(tablename: str, list_of_keys: list[str]) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads data table from filename\n",
    "    Keeps only specified keys (columns)\n",
    "    Replace each empty string with a NaN value\n",
    "    Converts all columns (except those in join_keys) to float\n",
    "    Deletes rows that include NaN values.\n",
    "    \"\"\"\n",
    "    df: pd.core.frame.DataFrame = csv_file_to_dataframe(os.path.join(core_directory, tablename))[\n",
    "        list_of_keys\n",
    "    ]\n",
    "    df.replace(\"\", pd.NA, inplace=True)\n",
    "    for col in list_of_keys:\n",
    "        if col not in join_keys:\n",
    "            df[col] = df[col].astype(float)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_dataframes_for_keys(\n",
    "    confounding_vars: list[tuple[str, list[str]]]\n",
    ") -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    This routine merges data tables for the confounding variables into a single table.\n",
    "\n",
    "    For each data table and its list of keys:\n",
    "        get it as a dataframe\n",
    "    Merge these dataframes into a single table using the join_keys\n",
    "    \"\"\"\n",
    "    df_generator: pd.core.frame.DataFrame = (\n",
    "        get_table_drop_nulls(tablename, [*join_keys, *list_of_keys])\n",
    "        for tablename, list_of_keys in confounding_vars\n",
    "        if list_of_keys\n",
    "    )\n",
    "    df_all_keys: pd.core.frame.DataFrame = next(df_generator)\n",
    "    for df_next in df_generator:\n",
    "        df_all_keys = pd.merge(\n",
    "            df_all_keys, df_next, on=join_keys, how=\"inner\", validate=\"one_to_one\"\n",
    "        )\n",
    "    return df_all_keys\n",
    "\n",
    "\n",
    "def merge_confounding_table(\n",
    "    confounding_vars: list[tuple[str, list[str]]], coregistered_images_directory: str\n",
    ") -> tuple[pd.core.frame.DataFrame, list[str]]:\n",
    "    \"\"\"\n",
    "    This creates the master data table from disparate sources that includes confounding variables and image meta data.\n",
    "    It also returns a list of the confounding variable names\n",
    "\n",
    "    Create a dataframe that contains all confounding variables (regardless of their source data table)\n",
    "    Create a dataframe that contains meta data about the available images\n",
    "    Merge these two dataframes using the join_keys\n",
    "    Note that a (src_subject_id, eventname) pair can occur more than once, e.g., for both \"md\" and \"fa\" image subtypes\n",
    "    \"\"\"\n",
    "    df_all_keys: pd.core.frame.DataFrame = merge_dataframes_for_keys(confounding_vars)\n",
    "    confounding_keys: list[str] = list(set(df_all_keys.columns).difference(set(join_keys)))\n",
    "\n",
    "    list_of_image_files: list[str] = get_list_of_image_files(coregistered_images_directory)\n",
    "    df_image_information: pd.core.frame.DataFrame = parse_image_filenames(list_of_image_files)\n",
    "    df_all_images: pd.core.frame.DataFrame = pd.merge(\n",
    "        df_all_keys,\n",
    "        df_image_information[[*join_keys, \"image_subtype\", \"filename\"]],\n",
    "        on=join_keys,\n",
    "        how=\"inner\",\n",
    "        validate=\"one_to_many\",\n",
    "    )\n",
    "    return df_all_images, confounding_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e25712-86cc-46b2-80dc-9bf912f9eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: describe this function\n",
    "# How we might process the inputs using numpy\n",
    "def use_numpy(\n",
    "    white_matter_mask: np.ndarray,\n",
    "    confounding_table: pd.core.frame.DataFrame,\n",
    "    interesting_ksads: list[str],\n",
    "    tested_vars: pd.core.frame.DataFrame,\n",
    "    confounding_keys: list[str],\n",
    ") -> dict[str, dict[str, np.ndarray]]:\n",
    "    image_subtypes: list[str] = list(confounding_table[\"image_subtype\"].unique())\n",
    "    # an_image_filename = confounding_table[\"filename\"].iloc[0]\n",
    "    # an_image_shape = get_data_from_image_files([an_image_filename])[0][1].shape\n",
    "\n",
    "    all_subtypes: dict[str, dict[str, np.ndarray]] = {}\n",
    "    for image_subtype in image_subtypes:\n",
    "        print(f\"  {image_subtype = }\")\n",
    "        subtype_information: pd.core.frame.DataFrame = confounding_table[\n",
    "            confounding_table[\"image_subtype\"] == image_subtype\n",
    "        ]\n",
    "        dict_of_images: dict[str, np.ndarray] = {\n",
    "            a: b\n",
    "            for a, b, c, d in get_data_from_image_files(\n",
    "                list(subtype_information[\"filename\"].values)\n",
    "            )\n",
    "        }\n",
    "        all_ksads_keys: dict[str, np.ndarray] = {}\n",
    "        for ksads_key in interesting_ksads:\n",
    "            print(f\"    {ksads_key = }\")\n",
    "            # Process only those images for which we have information for this ksads_key\n",
    "            augmented_information: pd.core.frame.DataFrame = pd.merge(\n",
    "                subtype_information,\n",
    "                tested_vars[[*join_keys, ksads_key]],\n",
    "                on=join_keys,\n",
    "                how=\"inner\",\n",
    "                validate=\"one_to_one\",\n",
    "            )\n",
    "            augmented_information.dropna(inplace=True)\n",
    "            augmented_information[\"constant\"] = 1.0\n",
    "\n",
    "            augmented_information.dropna(inplace=True)\n",
    "            print(f\"    {augmented_information.columns = }\")\n",
    "            print(f\"    {len(augmented_information) = }\")\n",
    "\n",
    "            X: np.ndarray = augmented_information[[*confounding_keys, ksads_key]].to_numpy()\n",
    "            kernel: np.ndarray = np.linalg.inv(X.transpose().dot(X))\n",
    "\n",
    "            # Now that we know which images we'll need, let's stack them into a single 4-dimensional shape\n",
    "            all_images: np.ndarray = np.stack(\n",
    "                [dict_of_images[filename] for filename in augmented_information[\"filename\"].values]\n",
    "            )\n",
    "            y: np.ndarray = all_images.reshape(all_images.shape[0], -1)[:, white_matter_mask]\n",
    "\n",
    "            print(f\"    {X.shape = }\")\n",
    "            print(f\"    {kernel.shape = }\")\n",
    "            print(f\"    {y.shape = }\")\n",
    "\n",
    "            X_T_Y: np.ndarray = X.transpose().dot(y)\n",
    "            sum_of_squares: np.ndarray = (y * y).sum(axis=0) - (X_T_Y * kernel.dot(X_T_Y)).sum(\n",
    "                axis=0\n",
    "            )\n",
    "            print(f\"    {sum_of_squares.shape = }\")\n",
    "            # TODO: Do we need to make sure that this background (zeros) is worse than foreground?\n",
    "            output_image: np.ndarray = np.zeros(white_matter_mask.shape)\n",
    "            output_image[white_matter_mask] = -sum_of_squares\n",
    "            output_image = output_image.reshape(all_images.shape[1:])\n",
    "\n",
    "            all_ksads_keys[ksads_key] = output_image\n",
    "        all_subtypes[image_subtype] = all_ksads_keys\n",
    "    return all_subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557f322-b929-4a6e-8c5a-1fb184c55eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: describe this function\n",
    "# How we might process the inputs using numpy\n",
    "def use_nilearn(\n",
    "    white_matter_mask: nibabel.nifti1.Nifti1Image,\n",
    "    confounding_table: pd.core.frame.DataFrame,\n",
    "    interesting_ksads: list[str],\n",
    "    tested_vars: pd.core.frame.DataFrame,\n",
    "    confounding_keys: list[str],\n",
    ") -> dict[str, dict[str, np.ndarray]]:\n",
    "    # print(f\"{white_matter_mask = }\")\n",
    "    # # confounding_table has columns *counfounding_vars, src_subject_id, eventname, image_subtype, filename\n",
    "    # print(f\"{confounding_table = }\")\n",
    "    # print(f\"{interesting_ksads = }\")  # list of str of selected ksads\n",
    "    # print(f\"{tested_vars = }\")  # Dataframe across all ksads\n",
    "    # print(f\"{confounding_keys = }\")  # list of str of confounding keys\n",
    "\n",
    "    image_subtypes = list(confounding_table[\"image_subtype\"].unique())\n",
    "    # an_image_filename = confounding_table[\"filename\"].iloc[0]\n",
    "    # an_image_shape = get_data_from_image_files([an_image_filename])[0][1].shape\n",
    "\n",
    "    all_subtypes = {}\n",
    "    for image_subtype in image_subtypes:\n",
    "        print(f\"{image_subtype = }\")\n",
    "\n",
    "        all_information = pd.merge(\n",
    "            confounding_table[confounding_table[\"image_subtype\"] == image_subtype],\n",
    "            tested_vars[[*join_keys, *interesting_ksads]],\n",
    "            on=join_keys,\n",
    "            how=\"inner\",\n",
    "            validate=\"one_to_one\",\n",
    "        )\n",
    "        all_information.dropna(inplace=True)\n",
    "\n",
    "        \"\"\"\n",
    "        Although pandas is a great way to read and manipulate these data tables, nilearn expects them to be \"array-like\".\n",
    "        Because panda tables require `.iloc` to accept integer coordinates for a 2d-table, panda tables fail \"array-like\".\n",
    "        We'll use numpy arrays.\n",
    "        \"\"\"\n",
    "        # tested_input: pd.core.frame.DataFrame = all_information[interesting_ksads]\n",
    "        tested_input: np.ndarray = all_information[interesting_ksads].to_numpy(dtype=float)\n",
    "        print(f\"  {tested_input.shape = }\")\n",
    "        # confounding_input: pd.core.frame.DataFrame = all_information[confounding_keys]\n",
    "        confounding_input: np.ndarray = all_information[confounding_keys].to_numpy(dtype=float)\n",
    "        print(f\"  {confounding_input.shape = }\")\n",
    "        # target_input: pd.core.frame.DataFrame = pd.DataFrame(\n",
    "        #     [\n",
    "        #         np_voxels.reshape(-1)[white_matter_mask]\n",
    "        #         for a, np_voxels, c, d in get_data_from_image_files(\n",
    "        #             list(all_information[\"filename\"].values)\n",
    "        #         )\n",
    "        #     ]\n",
    "        # )\n",
    "        target_input: np.ndarray = np.stack(\n",
    "            [\n",
    "                np_voxels.reshape(-1)\n",
    "                for a, np_voxels, c, d in get_data_from_image_files(\n",
    "                    list(all_information[\"filename\"].values)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print(f\"  {target_input.shape = }\")\n",
    "\n",
    "        model_intercept: bool = True\n",
    "        n_perm: int = 100  # TODO: Use 10000\n",
    "        two_sided_test: bool = True\n",
    "        random_state = None\n",
    "        n_jobs: int = 1  # TODO: Use -1\n",
    "        verbose: int = 1\n",
    "        masker = white_matter_mask\n",
    "        tfce: bool = False  # TODO: Set to True\n",
    "        threshold = None\n",
    "        output_type: str = \"dict\"\n",
    "        response: dict[str, np.ndarray] = nilearn.mass_univariate.permuted_ols(\n",
    "            tested_vars=tested_input,\n",
    "            target_vars=target_input,\n",
    "            confounding_vars=confounding_input,\n",
    "            model_intercept=model_intercept,\n",
    "            n_perm=n_perm,\n",
    "            two_sided_test=two_sided_test,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "            verbose=verbose,\n",
    "            masker=masker,\n",
    "            tfce=tfce,\n",
    "            threshold=threshold,\n",
    "            output_type=output_type,\n",
    "        )\n",
    "        all_subtypes[image_subtype] = response\n",
    "    return all_subtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f276d607-d067-4e85-acd3-2c8dcd6a9b58",
   "metadata": {},
   "source": [
    "## Define or load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4624d539-d5db-42cb-a8b3-50da7d04bcf7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Set inputs for our task.  Ultimately these will either be class members or parameters for class methods.\n",
    "\n",
    "# confounding_vars is the locations of some useful csv data columns. These files live in `core_directory`\n",
    "confounding_vars_input: list[tuple[str, list[str]]] = [\n",
    "    (\n",
    "        \"abcd-general/abcd_y_lt.csv\",\n",
    "        [\n",
    "            # TODO: Convert site_id_l, modified_rel_family_id, and demo_gender_id_v2 to one-hot so that we can use them.\n",
    "            # Currently they are str, Optional[int], and int.\n",
    "            # \"site_id_l\",  # Site ID at each event\n",
    "            # TODO: We are including participants with no siblings in study at the expense of losing family ID.  Do this better.\n",
    "            # \"rel_family_id\",  # Participants belonging to the same family share a family ID.  They will differ between data releases\n",
    "            \"interview_age\"  # Participant's age in month at start of the event\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"gender-identity-sexual-health/gish_p_gi.csv\",\n",
    "        [\n",
    "            # TODO: demo_gender_id_v2 should be one-hot, with handling for 777 and 999 as category=\"unknown\"?\n",
    "            \"demo_gender_id_v2\",  # 1=Male; 2=Female; 3=Trans male; 4=Trans female; 5=Gender queer; 6=Different; 777=Decline to answer; 999=Don't know\n",
    "            # \"demo_gender_id_v2_l\",  # same?, so don't include it\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"abcd-general/abcd_p_demo.csv\",\n",
    "        [\n",
    "            # TODO: These duplicate each other and gender-identity-sexual-health/gish_p_gi.csv.  Why?\n",
    "            # \"demo_gender_id_v2\",  # same?, so don't include it\n",
    "            # \"demo_gender_id_v2_l\",  # same?, so don't include it\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"physical-health/ph_y_bld.csv\",\n",
    "        [\n",
    "            # TODO: Can any of these be useful, or are they sparse/empty of useful information?\n",
    "            # \"biospec_blood_baso_percent\",  # BASO %\n",
    "            # \"biospec_blood_baso_abs\",  # BASO ABS\n",
    "            # \"biospec_blood_eos_percent\",  # EOS %\n",
    "            # \"biospec_blood_eos_abs\",  # EOS ABS\n",
    "            # \"biospec_blood_hemoglobin\",  # Hemoglobin\n",
    "            # \"biospec_blood_mcv\",  # MCV\n",
    "            # \"biospec_blood_plt_count\",  # PLT Count\n",
    "            # \"biospec_blood_wbc_count\",  # WBC Count\n",
    "            # \"biospec_blood_ferritin\",  # Ferritin\n",
    "            # \"biospec_blood_hemoglobin_a1\",  # hemoglobin_a1\n",
    "            # \"biospec_blood_imm_gran_per\",  # Immature Gran %\n",
    "        ],\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3cedda-1054-4507-83aa-1d30eb131e05",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Load tabular information for counfounding variables\n",
    "# Merge the information with image meta data so that we know which images we have confounding variable values for.\n",
    "start = time.time()\n",
    "confounding_table_input, confounding_keys_input = merge_confounding_table(\n",
    "    confounding_vars_input, coregistered_images_directory\n",
    ")\n",
    "print(f\"{confounding_keys_input = }\")\n",
    "print(f\"{len(confounding_table_input) = }\")\n",
    "print(f\"Total time to load confounding information = {time.time() - start}s\")\n",
    "\n",
    "# Load KSADS information\n",
    "file_mh_y_ksads_ss_input, interesting_ksads_input = find_interesting_ksads()\n",
    "tested_vars_input = ksads_filename_to_dataframe(file_mh_y_ksads_ss_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f997f-bc4c-4838-a5be-26ac6f55409b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a167d-ec31-474d-83ce-de54d923edcf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "mask_threshold: float = 0.70\n",
    "if False:\n",
    "    print(\"Invoking use_numpy\")\n",
    "    white_matter_mask_input = get_white_matter_mask_as_numpy(white_matter_mask_file, mask_threshold)\n",
    "    func = use_numpy\n",
    "else:\n",
    "    print(\"Invoking use_nilearn\")\n",
    "    white_matter_mask_input = nilearn.masking.compute_brain_mask(\n",
    "        target_img=white_matter_mask_file,\n",
    "        threshold=mask_threshold,\n",
    "        connected=False,  # TODO: Is this best?\n",
    "        opening=False,  # False or positive int\n",
    "        memory=None,\n",
    "        verbose=2,\n",
    "        mask_type=\"whole-brain\",  # \"whole-brain\", \"gm\", \"wm\"\n",
    "    )\n",
    "    print(f\"Number of white_matter voxels = {np.sum(white_matter_mask_input.get_fdata())}\")\n",
    "    print(\n",
    "        f\"Total number of voxels = {np.sum((white_matter_mask_input.get_fdata() == 0.0) | (white_matter_mask_input.get_fdata() == 1.0))}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"zero = {np.sum((white_matter_mask_input.get_fdata() != 0.0) & (white_matter_mask_input.get_fdata() != 1.0))}\"\n",
    "    )\n",
    "    func = use_nilearn\n",
    "\n",
    "start = time.time()\n",
    "output_images_by_subtype: dict[str, dict[str, np.ndarray]] = func(\n",
    "    white_matter_mask_input,\n",
    "    confounding_table_input,\n",
    "    interesting_ksads_input,\n",
    "    tested_vars_input,\n",
    "    confounding_keys_input,\n",
    ")\n",
    "print(f\"Computed all voxels in time {time.time() - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727eca10-18ca-4bd5-8ae6-aedacf37ca6a",
   "metadata": {},
   "source": [
    "## Show some output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595a5b5b-4ce7-462e-bc7f-cb2fa9cf9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{list(output_images_by_subtype.keys()) = }\")\n",
    "print(f\"{list(output_images_by_subtype['fa'].keys()) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233859af-1087-49a3-9a05-d837d3352e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if func == use_numpy:\n",
    "    print(\"## use_numpy output\")\n",
    "    print(f\"{type(output_images_by_subtype['fa']['ksads_1_187_t']) = }\")\n",
    "    means = np.array(\n",
    "        [\n",
    "            [\n",
    "                float(np.mean(value1.reshape(-1)[white_matter_mask_input]))\n",
    "                for key1, value1 in value0.items()\n",
    "            ]\n",
    "            for key0, value0 in output_images_by_subtype.items()\n",
    "        ]\n",
    "    )\n",
    "    print(f\"{means = }\")\n",
    "    stds = np.array(\n",
    "        [\n",
    "            [\n",
    "                float(np.std(value1.reshape(-1)[white_matter_mask_input]))\n",
    "                for key1, value1 in value0.items()\n",
    "            ]\n",
    "            for key0, value0 in output_images_by_subtype.items()\n",
    "        ]\n",
    "    )\n",
    "    print(f\"{stds = }\")\n",
    "    print(f\"Relative std = {stds/means!r}\")\n",
    "else:\n",
    "    print(\"Skipped use_numpy output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24f82a-7b07-4929-9b84-2cbf7bc70f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if func == use_nilearn:\n",
    "    print(\"## use_nilearn output\")\n",
    "    for sub_type in (\"fa\", \"md\"):\n",
    "        print(f\"## {sub_type = }\")\n",
    "        for table in output_images_by_subtype[sub_type].keys():\n",
    "            print(f\"## table: {table = }\")\n",
    "            print(f\"{output_images_by_subtype[sub_type][table].shape = }\")\n",
    "            print(f\"{np.sum(output_images_by_subtype[sub_type][table]) = }\")\n",
    "            print(f\"{np.sum(~np.isnan(output_images_by_subtype[sub_type][table])) = }\")\n",
    "            # print(f\"{output_images_by_subtype[sub_type]['t'] = }\")\n",
    "            # print(f\"{output_images_by_subtype[sub_type]['logp_max_t'] = }\")\n",
    "            # print(f\"{output_images_by_subtype[sub_type]['h0_max_t'] = }\")\n",
    "else:\n",
    "    print(\"Skipped use_nilearn output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d1747-a4db-4a3a-866c-ac1831084ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "abcd311",
   "language": "python",
   "name": "abcd311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
