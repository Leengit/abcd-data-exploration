{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b62a670",
   "metadata": {},
   "source": [
    "# An example workflow for generating hypotheses with ABCD data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388451e-8b5f-48fc-8c17-08eadc8cf694",
   "metadata": {},
   "source": [
    "## Installing dependencies\n",
    "One way to do this is with a virtual python environment installed so that it is accessible to Jupyter lab.  This needs to be set up only once.\n",
    "```bash\n",
    "python -m venv ~/abcd311\n",
    "source ~/abcd311/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name abcd311\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Then, once Jupyter is open with this lab notebook, \"Change Kernel...\" to be \"abcd311\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18a451-440b-456d-a76c-669d98d352f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from Python packages\n",
    "from dipy.io.image import load_nifti, save_nifti\n",
    "import csv\n",
    "import functools\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc957ff-2517-43a3-833a-472feef14e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global parameters to match your environment\n",
    "gor_image_directory = \"/data2/ABCD/gor-images\"\n",
    "coregistered_images_directory = os.path.join(gor_image_directory, \"coregistered-images\")\n",
    "tabular_data_directory = \"/data2/ABCD/abcd-5.0-tabular-data-extracted\"\n",
    "core_directory = os.path.join(tabular_data_directory, \"core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555acc8f-2c47-4aae-b015-e4bc85ea683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The locations of some useful csv data columns\n",
    "\n",
    "# These files live in `core_directory`\n",
    "\n",
    "independent_vars = [\n",
    "    [\n",
    "        \"abcd-general/abcd_y_lt.csv\",\n",
    "        [\n",
    "            \"site_id_l\",  # Site ID at each event\n",
    "            # TODO: We are including participants with no siblings in study at the expense of losing family ID.  Do this better.\n",
    "            # \"rel_family_id\",  # Participants belonging to the same family share a family ID.  They will differ between data releases\n",
    "            \"interview_age\",  # Participant's age in month at start of the event\n",
    "        ],\n",
    "    ],\n",
    "    [\n",
    "        \"abcd-general/abcd_p_demo.csv\",\n",
    "        [\n",
    "            \"demo_gender_id_v2\",  # 1=Male; 2=Female; 3=Trans male; 4=Trans female; 5=Gender queer; 6=Different; 777=Decline to answer; 999=Don't know\n",
    "            # \"demo_gender_id_v2_l\",  # same?\n",
    "        ],\n",
    "    ],\n",
    "    [\n",
    "        \"gender-identity-sexual-health/gish_p_gi.csv\",\n",
    "        [\n",
    "            \"demo_gender_id_v2\",  # 1=Male; 2=Female; 3=Trans male; 4=Trans female; 5=Gender queer; 6=Different; 777=Decline to answer; 999=Don't know\n",
    "            # \"demo_gender_id_v2_l\",  # same?\n",
    "        ],\n",
    "    ],\n",
    "    [\n",
    "        \"physical-health/ph_y_bld.csv\",\n",
    "        [\n",
    "            # \"biospec_blood_baso_percent\",  # BASO %\n",
    "            # \"biospec_blood_baso_abs\",  # BASO ABS\n",
    "            # \"biospec_blood_eos_percent\",  # EOS %\n",
    "            # \"biospec_blood_eos_abs\",  # EOS ABS\n",
    "            # \"biospec_blood_hemoglobin\",  # Hemoglobin\n",
    "            # \"biospec_blood_mcv\",  # MCV\n",
    "            # \"biospec_blood_plt_count\",  # PLT Count\n",
    "            # \"biospec_blood_wbc_count\",  # WBC Count\n",
    "            # \"biospec_blood_ferritin\",  # Ferritin\n",
    "            # \"biospec_blood_hemoglobin_a1\",  # hemoglobin_a1\n",
    "            # \"biospec_blood_imm_gran_per\",  # Immature Gran %\n",
    "        ],\n",
    "    ],\n",
    "]\n",
    "\n",
    "# We'll do KSADS variables once we've computed `interesting_ksads` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bb532-c965-4b78-8227-10ae8e726d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for handling image voxel data\n",
    "\n",
    "\n",
    "def get_list_of_image_files(directory):\n",
    "    response = [\n",
    "        os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".nii.gz\")\n",
    "    ]\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_image_filenames(list_of_image_files):\n",
    "    \"\"\"\n",
    "    Returns a pandas DataFrame.\n",
    "    The first column is the filename.  Additional columns indicate how the filename was parsed.\n",
    "    For example, run as:\n",
    "        df = parse_image_filenames(get_list_of_image_files(coregistered_images_directory))\n",
    "    \"\"\"\n",
    "    filename_pattern = r\"gorinput([0-9]+)-modality([0-9]+)-sub-([A-Za-z0-9]+)_ses-([A-Za-z0-9]+)_run-([A-Za-z0-9]+)_([A-Za-z0-9]+)_([A-Za-z0-9]+)-([A-Za-z0-9]+).nii.gz\"\n",
    "    filename_keys = [\n",
    "        \"filename\",\n",
    "        \"gorinput\",\n",
    "        \"modality\",\n",
    "        \"src_subject_id\",\n",
    "        \"eventname\",\n",
    "        \"run\",\n",
    "        \"image_type\",\n",
    "        \"image_subtype\",\n",
    "        \"processing\",\n",
    "    ]\n",
    "\n",
    "    response = pd.DataFrame(\n",
    "        [\n",
    "            [filename, *list(re.match(filename_pattern, os.path.basename(filename)).groups())]\n",
    "            for filename in list_of_image_files\n",
    "        ],\n",
    "        columns=filename_keys,\n",
    "    )\n",
    "    # Fix parsing of src_subject_id\n",
    "    response[\"src_subject_id\"] = [\n",
    "        re.sub(r\"^NDAR\", \"NDAR_\", subject) for subject in response[\"src_subject_id\"]\n",
    "    ]\n",
    "    # Fix parsing of eventname\n",
    "    eventname_conversion = {\n",
    "        \"baselineYear1Arm1\": \"baseline_year_1_arm_1\",\n",
    "        \"1YearFollowUpYArm1\": \"1_year_follow_up_y_arm_1\",\n",
    "        \"2YearFollowUpYArm1\": \"2_year_follow_up_y_arm_1\",\n",
    "        \"3YearFollowUpYArm1\": \"3_year_follow_up_y_arm_1\",\n",
    "        \"4YearFollowUpYArm1\": \"4_year_follow_up_y_arm_1\",\n",
    "    }\n",
    "    response[\"eventname\"] = [eventname_conversion[event] for event in response[\"eventname\"]]\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_data_from_image_files(list_of_files):\n",
    "    \"\"\"\n",
    "    get_data_from_image_files returns a list of tuples of 4 values:\n",
    "    <class 'str'>: full file name\n",
    "    <class 'numpy.ndarray'>: image data as a numpy array\n",
    "    <class 'numpy.ndarray'>: some other numpy array (transform?)\n",
    "    <class 'nibabel.nifti1.Nifti1Image'>: an object that can be modified and written to file as a .nii.gz file\n",
    "    \"\"\"\n",
    "    response = [(file,) + load_nifti(file, return_img=True) for file in list_of_files]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4ff2a-16c3-4183-85f0-35f0d3647c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reading and selecting data from csv files\n",
    "\n",
    "\n",
    "def csv_file_to_dataframe(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "# Select rows from data frame, to handle a common case where the direct pandas interface would be complicated\n",
    "def select_rows_of_dataframe(df, query_dict):\n",
    "    # Each key of query_dict is a column header of the df dataframe.\n",
    "    # Each value of query_dict is a list of allowed values.\n",
    "    # A row will be selected only if each of these columns has one of the allowed keys\n",
    "    assert all(key in df.columns for key in query_dict.keys())\n",
    "    # Old code: each of query_dict.values() is just a single value, not a list of values:\n",
    "    # rows = df[\n",
    "    #     functools.reduce(lambda x, y: x & y, [df[key] = value for key, value in query_dict.items()])\n",
    "    # ]\n",
    "    rows = df[\n",
    "        functools.reduce(\n",
    "            lambda x, y: x & y,\n",
    "            [\n",
    "                functools.reduce(lambda x, y: x | y, [df[key] == value for value in values])\n",
    "                for key, values in query_dict.items()\n",
    "            ],\n",
    "        )\n",
    "    ]\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feed3d2-6580-4e09-9dfe-cfb78e2e1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for computing summary statistics for KSADS csv data\n",
    "\n",
    "\n",
    "def data_frame_value_counts(df):\n",
    "    # Returns a dict:\n",
    "    #     Each key is a column name\n",
    "    #     Each value is a dict:\n",
    "    #         Each key of this is a value that occurs in the column.\n",
    "    #         The corresponding value is the number of occurrences.\n",
    "    for column in df.columns:\n",
    "        if bool(re.match(\"ksads_\\d\", column)):\n",
    "            df.loc[df[column] == 555, column] = np.nan\n",
    "            df.loc[df[column] == 888, column] = 0\n",
    "    return {\n",
    "        column: dict(df[column].value_counts(dropna=False).astype(int)) for column in df.columns\n",
    "    }\n",
    "\n",
    "\n",
    "def entropy_of_column_counts(column_counts):\n",
    "    assert all(value >= 0 for value in column_counts.values())\n",
    "    total_count = sum(column_counts.values())\n",
    "    entropy = sum(\n",
    "        [\n",
    "            count / total_count * math.log2(total_count / count)\n",
    "            for count in column_counts.values()\n",
    "            if count > 0\n",
    "        ]\n",
    "    )\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def ksads_keys_only(all_columns):\n",
    "    return {key: value for key, value in all_columns.items() if bool(re.match(\"ksads_\\d\", key))}\n",
    "\n",
    "\n",
    "def entropy_of_all_columns(all_columns):\n",
    "    return {key: entropy_of_column_counts(value) for key, value in all_columns.items()}\n",
    "\n",
    "\n",
    "def find_interesting_entropies(file_mh_y_ksads_ss):\n",
    "    # Find some KSADS data columns with high entropy.\n",
    "    df_mh_y_ksads_ss = csv_file_to_dataframe(file_mh_y_ksads_ss)\n",
    "    print(\"Read done\")\n",
    "\n",
    "    counts_for_each_column_mh_y_ksads_ss = ksads_keys_only(\n",
    "        data_frame_value_counts(df_mh_y_ksads_ss)\n",
    "    )\n",
    "    print(\"Column counting done\")\n",
    "\n",
    "    entropies = entropy_of_all_columns(counts_for_each_column_mh_y_ksads_ss)\n",
    "    sorted_entropies = dict(sorted(entropies.items(), key=lambda item: item[1], reverse=True))\n",
    "    sorted_entropies = {\n",
    "        key: (value, counts_for_each_column_mh_y_ksads_ss[key])\n",
    "        for key, value in sorted_entropies.items()\n",
    "        # if bool(re.match(\"ksads_\\d\", key))\n",
    "    }\n",
    "    print(\"Entropy calculation done\")\n",
    "    return sorted_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d841f51-7690-4152-8e78-0bff309e4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find interesting KSADS data\n",
    "\n",
    "file_mh_y_ksads_ss = \"mental-health/mh_y_ksads_ss.csv\"\n",
    "if True:\n",
    "    # User requests that we re-compute `interesting_ksads`\n",
    "    if False:\n",
    "        # User requests that we re-compute `sorted_entropies`\n",
    "        try:\n",
    "            del sorted_entropies\n",
    "        except NameError:\n",
    "            pass\n",
    "    try:\n",
    "        sorted_entropies\n",
    "        print(\"Using cached value for sorted_entropies\")\n",
    "    except NameError:\n",
    "        full_path = os.path.join(core_directory, file_mh_y_ksads_ss)\n",
    "        # find_interesting_entropies is slow\n",
    "        sorted_entropies = find_interesting_entropies(full_path)\n",
    "    number_wanted = 20\n",
    "    print(f\"Distribution details: {list(sorted_entropies.items())[:number_wanted]}\")\n",
    "    interesting_ksads = list(sorted_entropies.keys())[:number_wanted]\n",
    "    print(f\"{interesting_ksads = }\")\n",
    "else:\n",
    "    # With \"555\" and \"888\" both going to \"\", the interesting_ksads computation gives:\n",
    "    interesting_ksads = [\n",
    "        \"ksads_22_142_t\",  # Symptom - Insomnia, Past\n",
    "        \"ksads_22_970_t\",  # Diagnosis - SLEEP PROBLEMS, Past\n",
    "        \"ksads_2_11_t\",  # Symptom - Explosive Irritability, Past\n",
    "        \"ksads_1_2_t\",  # Symptom - Depressed Mood, Past\n",
    "        \"ksads_2_13_t\",  # Symptom - Decreased Need for Sleep, Past\n",
    "        \"ksads_1_6_t\",  # Symptom - Anhedonia, Past\n",
    "        \"ksads_22_141_t\",  # Symptom - Insomnia, Present\n",
    "        \"ksads_22_969_t\",  # Diagnosis - SLEEP PROBLEMS, Present\n",
    "        \"ksads_2_8_t\",  # Symptom - Elevated Mood, Past\n",
    "        \"ksads_10_46_t\",  # Symptom - Excessive worries more days than not Past\n",
    "        \"ksads_1_4_t\",  # Symptom - Irritability, Past\n",
    "        \"ksads_2_10_t\",  # Symptom - ExplosiveIrritability, PresentNext\n",
    "        \"ksads_8_31_t\",  # Symptom - Fear of Social Situations, Past\n",
    "        \"ksads_23_146_t\",  # Symptom - Wishes/Better off dead, Past\n",
    "        \"ksads_23_957_t\",  # Diagnosis - SuicidalideationPassivePast\n",
    "        \"ksads_1_5_t\",  # Symptom - Anhedonia, Present\n",
    "        \"ksads_2_839_t\",  # Diagnosis - Unspecified Bipolar and Related Disorder, PAST (F31.9)\n",
    "        \"ksads_2_833_t\",  # Diagnosis - Bipolar I Disorder, most recent past episode manic (F31.1x)\n",
    "        \"ksads_1_3_t\",  # Symptom - Irritability, Present\n",
    "        \"ksads_1_842_t\",  # Diagnosis - Major Depressive Disorder, Past (F32.9)\n",
    "    ]\n",
    "    # With \"555\" going to \"\" and \"888\" going to \"0\", the interesting_ksads computation gives:\n",
    "    interesting_ksads = [\n",
    "        \"ksads_1_187_t\",  # Symptom - No two month symptom-free interval, Present\n",
    "        \"ksads_1_188_t\",  # Symptom - No two month symptom-free interval, Past\n",
    "        \"ksads_22_142_t\",  # Symptom - Insomnia, Past\n",
    "        \"ksads_22_970_t\",  # Diagnosis - SLEEP PROBLEMS, Past\n",
    "        \"ksads_2_11_t\",  # Symptom - Explosive Irritability, Past\n",
    "        \"ksads_2_222_t\",  # Symptom - Lasting at least 4 days, Past\n",
    "        \"ksads_1_184_t\",  # Symptom - Impairment in functioning due to depression, Past\n",
    "        \"ksads_1_2_t\",  # Symptom - Depressed Mood, Past\n",
    "        \"ksads_2_13_t\",  # Symptom - Decreased Need for Sleep, Past\n",
    "        \"ksads_1_6_t\",  # Symptom - Anhedonia, Past\n",
    "        \"ksads_1_160_t\",  # Symptom - Fatigue, Past\n",
    "        \"ksads_1_162_t\",  # Symptom - Concentration Disturbance, Past\n",
    "        \"ksads_2_220_t\",  # Symptom - Lasting at least one week, Past\n",
    "        \"ksads_1_156_t\",  # Symptom - Insomnia when depressed, Past\n",
    "        \"ksads_1_174_t\",  # Symptom - Psychomotor Agitation in Depressive Disorder, Past\n",
    "        \"ksads_2_216_t\",  # Symptom - Impairment in functioning due to bipolar, Past\n",
    "        \"ksads_2_208_t\",  # Symptom - Psychomotor Agitation in Bipolar Disorder, Past\n",
    "        \"ksads_2_206_t\",  # Symptom - Increased Energy, Past\n",
    "        \"ksads_22_141_t\",  # Symptom - Insomnia, Present\n",
    "        \"ksads_22_969_t\",  # Diagnosis - SLEEP PROBLEMS, Present\n",
    "    ]\n",
    "\n",
    "ksads_vars = [[file_mh_y_ksads_ss, interesting_ksads]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce856d-2f88-4b17-b4fc-17fcb052eb26",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Find all images for which we have tabular data\n",
    "\n",
    "join_keys = [\"src_subject_id\", \"eventname\"]\n",
    "\n",
    "\n",
    "def get_table_drop_nulls(tablename, list_of_keys):\n",
    "    df = csv_file_to_dataframe(os.path.join(core_directory, tablename))[list_of_keys]\n",
    "    df.replace(\"\", pd.NA, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    print(df.head())\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_dataframes_for_keys(independent_vars):\n",
    "    df_generator = (\n",
    "        get_table_drop_nulls(tablename, [*join_keys, *list_of_keys])\n",
    "        for tablename, list_of_keys in independent_vars\n",
    "        if list_of_keys\n",
    "    )\n",
    "    df_all_keys = next(df_generator)\n",
    "    for df_next in df_generator:\n",
    "        df_all_keys = pd.merge(\n",
    "            df_all_keys, df_next, on=join_keys, how=\"inner\", validate=\"one_to_one\"\n",
    "        )\n",
    "    return df_all_keys\n",
    "\n",
    "\n",
    "def merge_tabular_information(independent_vars, coregistered_images_directory):\n",
    "    df_all_keys = merge_dataframes_for_keys(independent_vars)\n",
    "\n",
    "    list_of_image_files = get_list_of_image_files(coregistered_images_directory)\n",
    "    df_image_information = parse_image_filenames(list_of_image_files)\n",
    "    df_all_images = pd.merge(\n",
    "        df_all_keys,\n",
    "        df_image_information[[*join_keys, \"image_subtype\"]],\n",
    "        on=join_keys,\n",
    "        how=\"inner\",\n",
    "        validate=\"one_to_many\",\n",
    "    )\n",
    "    return df_all_images\n",
    "\n",
    "\n",
    "tabular_information = merge_tabular_information(independent_vars, coregistered_images_directory)\n",
    "print(f\"{len(tabular_information) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57674d6e-f7e4-441c-99f7-57949fe641b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "abcd311",
   "language": "python",
   "name": "abcd311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
